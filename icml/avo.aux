\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{beaumont2002approximate,marjoram2003markov,sisson2007sequential,sisson2011likelihood,marin2012approximate,cranmer2015approximating}
\citation{DBLP:journals/corr/KingmaW13}
\citation{goodfellow2014generative}
\citation{2016arXiv161003483M}
\citation{goodfellow2014generative}
\citation{2011arXiv1106.4487W,2012arXiv1212.4507S}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem statement}{1}{section.2}}
\newlabel{sec:problem}{{2}{1}{}{section.2}{}}
\citation{goodfellow2014generative}
\citation{goodfellow2014generative}
\citation{2017arXiv170104862A}
\citation{2017arXiv170107875A}
\citation{2017arXiv170400028G}
\citation{2012arXiv1212.4507S,staines2013optimization}
\citation{2011arXiv1106.4487W}
\citation{2012arXiv1212.4507S}
\newlabel{eqn:p_theta}{{1}{2}{}{equation.2.1}{}}
\newlabel{eqn:p_x_sim}{{2}{2}{}{equation.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Generative adversarial networks}{2}{subsection.3.1}}
\newlabel{sec:gans}{{3.1}{2}{}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Variational optimization}{2}{subsection.3.2}}
\newlabel{eqn:approx-grad}{{8}{2}{}{equation.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Adversarial variational optimization}{2}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Algorithm}{2}{subsection.4.1}}
\citation{2014arXiv1412.6980K}
\citation{2011arXiv1106.4487W}
\citation{ranganath2014black,tran2017variational}
\citation{2016arXiv161009033R}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Adversarial variational optimization (AVO).}}{3}{algorithm.1}}
\newlabel{alg:avo}{{1}{3}{}{algorithm.1}{}}
\newlabel{eqn:vo-ud}{{9}{3}{}{equation.4.9}{}}
\newlabel{eqn:vo-ug}{{10}{3}{}{equation.4.10}{}}
\newlabel{eqn:grad-ug-approx}{{11}{3}{}{equation.4.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Parameter Point Estimates}{3}{subsection.4.2}}
\newlabel{eq:marginal_likelihood}{{13}{3}{}{equation.4.13}{}}
\citation{rubin1984}
\citation{cranmer2015approximating}
\newlabel{eqn:p_psi}{{14}{4}{}{equation.4.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{4}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Univariate discrete data}{4}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Multidimensional continuous data}{4}{subsection.5.2}}
\citation{goodfellow2014generative,2017arXiv170104862A,2017arXiv170107875A}
\citation{2012arXiv1212.4507S,staines2013optimization}
\citation{bernton2017inference}
\citation{montavon2016wasserstein}
\citation{cuturi2013sinkhorn,genevay2016stochastic,montavon2016wasserstein}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Discrete Poisson model with unknown mean. ({\it  Top left}) Proposal distributions $q({\bm  {\theta }}|{\bm  {\psi }})$ after training. For both $\gamma =0$ and $\gamma =5$, the distributions correctly concentrate their density around the true value $\qopname  \relax o{log}(\lambda ^*)$. Penalizing the entropy of the proposal distribution ($\gamma =5$) results in a tighter density. ({\it  Top right}) Model distributions $q(\mathbf  {x}|{\bm  {\psi }})$ after training. This plot shows that the resulting parametrizations of the simulator closely reproduce the true distribution. ({\it  Bottom}) Empirical estimates of the variational upper bound $U_d$ as optimization progresses. }}{5}{figure.1}}
\newlabel{fig:poisson}{{1}{5}{Discrete Poisson model with unknown mean. ({\it Top left}) Proposal distributions $q(\bftheta |\bfpsi )$ after training. For both $\gamma =0$ and $\gamma =5$, the distributions correctly concentrate their density around the true value $\log (\lambda ^*)$. Penalizing the entropy of the proposal distribution ($\gamma =5$) results in a tighter density. ({\it Top right}) Model distributions $\qxpsi $ after training. This plot shows that the resulting parametrizations of the simulator closely reproduce the true distribution. ({\it Bottom}) Empirical estimates of the variational upper bound $U_d$ as optimization progresses}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Multidimensional continuous data. ({\it  Left}) Density $q({\bm  {\theta }}|{\bm  {\psi }})$ at the beginning of the procedure, for a proposal distribution initialized with zero mean and unit variance. Contours correspond to parameters ${\bm  {\theta }}$ within $1$-$2$-$3$ Mahalanobis distance units from the mean of $q({\bm  {\theta }}|{\bm  {\psi }})$. ({\it  Right}) Density $q({\bm  {\theta }}|{\bm  {\psi }})$ after adversarial variational optimization ($\gamma =10$). The proposal density correctly converges towards a distribution whose density concentrates around ${\bm  {\theta }}^* = (1, -1)$. }}{5}{figure.2}}
\newlabel{fig:multi}{{2}{5}{Multidimensional continuous data. ({\it Left}) Density $q(\bftheta |\bfpsi )$ at the beginning of the procedure, for a proposal distribution initialized with zero mean and unit variance. Contours correspond to parameters $\bftheta $ within $1$-$2$-$3$ Mahalanobis distance units from the mean of $q(\bftheta |\bfpsi )$. ({\it Right}) Density $q(\bftheta |\bfpsi )$ after adversarial variational optimization ($\gamma =10$). The proposal density correctly converges towards a distribution whose density concentrates around $\bftheta ^* = (1, -1)$}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related work}{5}{section.6}}
\citation{2016arXiv161003483M}
\citation{gutmann2012noise,cranmer2015approximating,cranmer2016experiments,2016arXiv161110242D,gutmann2017likelihood,rosca2017variational}
\citation{meeds2015hamiltonian}
\citation{dumoulin2016adversarially}
\citation{donahue2016adversarial}
\citation{rosca2017variational}
\citation{DBLP:journals/corr/MeschederNG17}
\citation{2017arXiv170208235H}
\citation{tran2017variational}
\citation{2016arXiv161009033R}
\newlabel{eq:opvi_avo}{{17}{6}{}{equation.6.17}{}}
\newlabel{eq:opvi_avo2}{{18}{6}{}{equation.6.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Summary}{6}{section.7}}
\bibdata{bibliography.bib}
\bibcite{Agostinelli:2002hh}{{1}{2003}{{Agostinelli et~al.}}{{}}}
\bibcite{Alwall:2011uj}{{2}{2011}{{Alwall et~al.}}{{Alwall, Herquet, Maltoni, Mattelaer, and Stelzer}}}
\bibcite{2017arXiv170104862A}{{3}{2017}{{{Arjovsky} \& {Bottou}}}{{{Arjovsky} and {Bottou}}}}
\bibcite{2017arXiv170107875A}{{4}{2017}{{{Arjovsky} et~al.}}{{{Arjovsky}, {Chintala}, and {Bottou}}}}
\bibcite{beaumont2002approximate}{{5}{2002}{{Beaumont et~al.}}{{Beaumont, Zhang, and Balding}}}
\bibcite{bernton2017inference}{{6}{2017}{{Bernton et~al.}}{{Bernton, Jacob, Gerber, and Robert}}}
\bibcite{cranmer2016experiments}{{7}{2016}{{Cranmer et~al.}}{{Cranmer, Pavez, Louppe, and Brooks}}}
\bibcite{cranmer2015approximating}{{8}{2015}{{Cranmer et~al.}}{{Cranmer, Pavez, and Louppe}}}
\bibcite{cuturi2013sinkhorn}{{9}{2013}{{Cuturi}}{{}}}
\bibcite{donahue2016adversarial}{{10}{2016}{{Donahue et~al.}}{{Donahue, Kr{\"a}henb{\"u}hl, and Darrell}}}
\bibcite{dumoulin2016adversarially}{{11}{2016}{{Dumoulin et~al.}}{{Dumoulin, Belghazi, Poole, Lamb, Arjovsky, Mastropietro, and Courville}}}
\bibcite{2016arXiv161110242D}{{12}{2016}{{{Dutta} et~al.}}{{{Dutta}, {Corander}, {Kaski}, and {Gutmann}}}}
\bibcite{genevay2016stochastic}{{13}{2016}{{Genevay et~al.}}{{Genevay, Cuturi, Peyr{\'e}, and Bach}}}
\bibcite{goodfellow2014generative}{{14}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{2017arXiv170400028G}{{15}{2017}{{{Gulrajani} et~al.}}{{{Gulrajani}, {Ahmed}, {Arjovsky}, {Dumoulin}, and {Courville}}}}
\bibcite{gutmann2012noise}{{16}{2012}{{Gutmann \& Hyv{\"a}rinen}}{{Gutmann and Hyv{\"a}rinen}}}
\bibcite{gutmann2017likelihood}{{17}{2017}{{Gutmann et~al.}}{{Gutmann, Dutta, Kaski, and Corander}}}
\bibcite{2017arXiv170208235H}{{18}{2017}{{Husz{\'a}r}}{{}}}
\bibcite{2014arXiv1412.6980K}{{19}{2014}{{{Kingma} \& {Ba}}}{{{Kingma} and {Ba}}}}
\bibcite{DBLP:journals/corr/KingmaW13}{{20}{2013}{{Kingma \& Welling}}{{Kingma and Welling}}}
\bibcite{marin2012approximate}{{21}{2012}{{Marin et~al.}}{{Marin, Pudlo, Robert, and Ryder}}}
\bibcite{marjoram2003markov}{{22}{2003}{{Marjoram et~al.}}{{Marjoram, Molitor, Plagnol, and Tavar{\'e}}}}
\bibcite{meeds2015hamiltonian}{{23}{2015}{{Meeds et~al.}}{{Meeds, Leenders, and Welling}}}
\bibcite{DBLP:journals/corr/MeschederNG17}{{24}{2017}{{Mescheder et~al.}}{{Mescheder, Nowozin, and Geiger}}}
\bibcite{2016arXiv161003483M}{{25}{2016}{{{Mohamed} \& {Lakshminarayanan}}}{{{Mohamed} and {Lakshminarayanan}}}}
\bibcite{montavon2016wasserstein}{{26}{2016}{{Montavon et~al.}}{{Montavon, M{\"u}ller, and Cuturi}}}
\bibcite{2016arXiv161009033R}{{27}{2016}{{{Ranganath} et~al.}}{{{Ranganath}, {Altosaar}, {Tran}, and {Blei}}}}
\bibcite{ranganath2014black}{{28}{2014}{{Ranganath et~al.}}{{Ranganath, Gerrish, and Blei}}}
\bibcite{rosca2017variational}{{29}{2017}{{Rosca et~al.}}{{Rosca, Lakshminarayanan, Warde-Farley, and Mohamed}}}
\bibcite{rubin1984}{{30}{1984}{{Rubin}}{{}}}
\bibcite{sisson2011likelihood}{{31}{2011}{{Sisson \& Fan}}{{Sisson and Fan}}}
\bibcite{sisson2007sequential}{{32}{2007}{{Sisson et~al.}}{{Sisson, Fan, and Tanaka}}}
\bibcite{2012arXiv1212.4507S}{{33}{2012}{{{Staines} \& {Barber}}}{{{Staines} and {Barber}}}}
\bibcite{staines2013optimization}{{34}{2013}{{Staines \& Barber}}{{Staines and Barber}}}
\bibcite{tran2017variational}{{35}{2017}{{Tran et~al.}}{{Tran, Nott, and Kohn}}}
\bibcite{2011arXiv1106.4487W}{{36}{2011}{{{Wierstra} et~al.}}{{{Wierstra}, {Schaul}, {Glasmachers}, {Sun}, and {Schmidhuber}}}}
\bibstyle{icml2018}
\citation{Alwall:2011uj}
\citation{Agostinelli:2002hh}
\@writefile{toc}{\contentsline {section}{\numberline {A}Further experiments}{8}{appendix.A}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Electron--positron annihilation}{8}{subsection.A.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Electron--positron annihilation. ({\it  Top left}) Proposal distributions $q({\bm  {\theta }}|{\bm  {\psi }})$ after adversarial variational optimization. Contours correspond to parameters ${\bm  {\theta }}$ within $1$-$2$-$3$ Mahalanobis distance units from the mean of $q({\bm  {\theta }}|{\bm  {\psi }})$. The density of the penalized distribution ($\gamma =5$) is highly concentrated, resulting in the green mass near ${\bm  {\theta }}^*$. ({\it  Top right}) Model distributions $q(\mathbf  {x}|{\bm  {\psi }})$ after training. Despite the differences between their proposal distributions, both models closely match the observed data. ({\it  Bottom}) Empirical estimates of the variational upper bound $U_d$ as optimization progresses. }}{8}{figure.3}}
\newlabel{fig:weinberg}{{3}{8}{Electron--positron annihilation. ({\it Top left}) Proposal distributions $q(\bftheta |\bfpsi )$ after adversarial variational optimization. Contours correspond to parameters $\bftheta $ within $1$-$2$-$3$ Mahalanobis distance units from the mean of $q(\bftheta |\bfpsi )$. The density of the penalized distribution ($\gamma =5$) is highly concentrated, resulting in the green mass near $\bftheta ^*$. ({\it Top right}) Model distributions $\qxpsi $ after training. Despite the differences between their proposal distributions, both models closely match the observed data. ({\it Bottom}) Empirical estimates of the variational upper bound $U_d$ as optimization progresses}{figure.3}{}}
\newlabel{eq:simple_weinberg}{{19}{8}{}{equation.A.19}{}}
