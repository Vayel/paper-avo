\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{plainnat}
\citation{beaumont2002approximate,marjoram2003markov,sisson2007sequential,sisson2011likelihood,marin2012approximate,cranmer2015approximating}
\citation{DBLP:journals/corr/KingmaW13}
\citation{goodfellow2014generative}
\citation{2016arXiv161003483M}
\citation{goodfellow2014generative}
\citation{2011arXiv1106.4487W,2012arXiv1212.4507S}
\jmlr@author{author names withheld}{author names withheld}
\jmlr@workshop{31th Annual Conference on Learning Theory}
\jmlr@title{Adversarial Variational Optimization}{Adversarial Variational Optimization\\ of Non-Differentiable Simulators}
\newlabel{jmlrstart}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.0.1}}
\citation{goodfellow2014generative}
\citation{goodfellow2014generative}
\citation{2017arXiv170104862A}
\citation{2017arXiv170107875A}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem statement}{2}{section.0.2}}
\newlabel{sec:problem}{{2}{2}{Problem statement}{section.0.2}{}}
\newlabel{eqn:p_theta}{{1}{2}{Problem statement}{equation.0.2.1}{}}
\newlabel{eqn:p_x_sim}{{2}{2}{Problem statement}{equation.0.2.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background}{2}{section.0.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Generative adversarial networks}{2}{subsection.0.3.1}}
\newlabel{sec:gans}{{3.1}{2}{Generative adversarial networks}{subsection.0.3.1}{}}
\citation{2017arXiv170400028G}
\citation{2012arXiv1212.4507S,staines2013optimization}
\citation{2011arXiv1106.4487W}
\citation{2012arXiv1212.4507S}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Variational optimization}{3}{subsection.0.3.2}}
\newlabel{eqn:approx-grad}{{8}{3}{Variational optimization}{equation.0.3.8}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Adversarial variational optimization (AVO).}}{4}{algorithm.1}}
\newlabel{alg:avo}{{1}{4}{Algorithm}{algorithm.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Adversarial variational optimization}{4}{section.0.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Algorithm}{4}{subsection.0.4.1}}
\newlabel{eqn:vo-ud}{{9}{4}{Algorithm}{equation.0.4.9}{}}
\newlabel{eqn:vo-ug}{{10}{4}{Algorithm}{equation.0.4.10}{}}
\newlabel{eqn:grad-ug-approx}{{11}{4}{Algorithm}{equation.0.4.11}{}}
\citation{2014arXiv1412.6980K}
\citation{2011arXiv1106.4487W}
\citation{2017arXiv171100123G}
\citation{buesingstochastic}
\citation{2016arXiv161009033R}
\citation{rubin1984}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Parameter Point Estimates}{5}{subsection.0.4.2}}
\newlabel{eq:marginal_likelihood}{{13}{5}{Parameter Point Estimates}{equation.0.4.13}{}}
\newlabel{eqn:p_psi}{{14}{5}{Parameter Point Estimates}{equation.0.4.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Discrete Poisson model with unknown mean. ({\obsoletefontcs  {it}Top left}) Proposal distributions $q({\bm  {\theta }}|{\bm  {\psi }})$ after training. For both $\gamma =0$ and $\gamma =3$, the distributions correctly concentrate their density around the true value $\qopname  \relax o{log}(\lambda ^*)$. Entropic regularization ($\gamma =3$) results in a tighter density. ({\obsoletefontcs  {it}Top right}) Model distributions $q(\mathbf  {x}|{\bm  {\psi }})$ after training. This plot shows that the resulting parametrizations of the simulator closely reproduce the true distribution, with better results when enabling entropic regularization. ({\obsoletefontcs  {it}Bottom}) Empirical estimates of the variational upper bound $U_d$ as optimization progresses. }}{6}{figure.1}}
\newlabel{fig:poisson}{{1}{6}{Discrete Poisson model with unknown mean. ({\it Top left}) Proposal distributions $q(\bftheta |\bfpsi )$ after training. For both $\gamma =0$ and $\gamma =3$, the distributions correctly concentrate their density around the true value $\log (\lambda ^*)$. Entropic regularization ($\gamma =3$) results in a tighter density. ({\it Top right}) Model distributions $\qxpsi $ after training. This plot shows that the resulting parametrizations of the simulator closely reproduce the true distribution, with better results when enabling entropic regularization. ({\it Bottom}) Empirical estimates of the variational upper bound $U_d$ as optimization progresses}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{6}{section.0.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Univariate discrete data}{6}{subsection.0.5.1}}
\citation{cranmer2015approximating}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Multidimensional continuous data}{7}{subsection.0.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Electron--positron annihilation}{7}{subsection.0.5.3}}
\citation{Alwall:2011uj}
\citation{Agostinelli:2002hh}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Multidimensional continuous data. ({\obsoletefontcs  {it}Left}) Density $q({\bm  {\theta }}|{\bm  {\psi }})$ at the beginning of the procedure, for a proposal distribution initialized with zero mean and unit variance. Contours correspond to parameters ${\bm  {\theta }}$ within $1$-$2$-$3$ Mahalanobis distance units from the mean of $q({\bm  {\theta }}|{\bm  {\psi }})$. ({\obsoletefontcs  {it}Right}) Density $q({\bm  {\theta }}|{\bm  {\psi }})$ after adversarial variational optimization. The proposal density correctly converges towards a distribution whose density concentrates around ${\bm  {\theta }}^* = (1, -1)$. }}{8}{figure.2}}
\newlabel{fig:multi}{{2}{8}{Multidimensional continuous data. ({\it Left}) Density $q(\bftheta |\bfpsi )$ at the beginning of the procedure, for a proposal distribution initialized with zero mean and unit variance. Contours correspond to parameters $\bftheta $ within $1$-$2$-$3$ Mahalanobis distance units from the mean of $q(\bftheta |\bfpsi )$. ({\it Right}) Density $q(\bftheta |\bfpsi )$ after adversarial variational optimization. The proposal density correctly converges towards a distribution whose density concentrates around $\bftheta ^* = (1, -1)$}{figure.2}{}}
\newlabel{eq:simple_weinberg}{{16}{8}{Electron--positron annihilation}{equation.0.5.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Electron--positron annihilation. ({\obsoletefontcs  {it}Top left}) Proposal distributions $q({\bm  {\theta }}|{\bm  {\psi }})$ after adversarial variational optimization. Contours correspond to parameters ${\bm  {\theta }}$ within $1$-$2$-$3$ Mahalanobis distance units from the mean of $q({\bm  {\theta }}|{\bm  {\psi }})$. The density of the proposal distribution concentrates in the neighborhood of ${\bm  {\theta }}^*$. ({\obsoletefontcs  {it}Top right}) Model distribution $q(\mathbf  {x}|{\bm  {\psi }})$ after training. Despite the width of the proposal distribution with respect to ${\bm  {\theta }}^*$, the synthetic data distribution matches the observed data. ({\obsoletefontcs  {it}Bottom}) Empirical estimates of the variational upper bound $U_d$ as optimization progresses. }}{9}{figure.3}}
\newlabel{fig:weinberg}{{3}{9}{Electron--positron annihilation. ({\it Top left}) Proposal distributions $q(\bftheta |\bfpsi )$ after adversarial variational optimization. Contours correspond to parameters $\bftheta $ within $1$-$2$-$3$ Mahalanobis distance units from the mean of $q(\bftheta |\bfpsi )$. The density of the proposal distribution concentrates in the neighborhood of $\bftheta ^*$. ({\it Top right}) Model distribution $\qxpsi $ after training. Despite the width of the proposal distribution with respect to $\bftheta ^*$, the synthetic data distribution matches the observed data. ({\it Bottom}) Empirical estimates of the variational upper bound $U_d$ as optimization progresses}{figure.3}{}}
\citation{goodfellow2014generative,2017arXiv170104862A}
\citation{2012arXiv1212.4507S}
\citation{bernton2017inference}
\citation{montavon2016wasserstein}
\citation{cuturi2013sinkhorn,genevay2016stochastic,montavon2016wasserstein}
\citation{2016arXiv161003483M}
\citation{gutmann2012noise,cranmer2015approximating,cranmer2016experiments,2016arXiv161110242D,gutmann2017likelihood,rosca2017variational}
\citation{meeds2015hamiltonian}
\@writefile{toc}{\contentsline {section}{\numberline {6}Related work}{10}{section.0.6}}
\citation{dumoulin2016adversarially}
\citation{donahue2016adversarial}
\citation{rosca2017variational}
\citation{DBLP:journals/corr/MeschederNG17}
\citation{2017arXiv170208235H}
\citation{tran2017variational}
\citation{2017arXiv170208896T}
\citation{2016arXiv161003483M}
\citation{2017arXiv171203353M}
\citation{2016arXiv161009033R}
\newlabel{eq:opvi_avo}{{18}{11}{Related work}{equation.0.6.18}{}}
\newlabel{eq:opvi_avo2}{{19}{11}{Related work}{equation.0.6.18}{}}
\bibdata{bibliography}
\bibcite{Agostinelli:2002hh}{{1}{2003}{{Agostinelli et~al.}}{{}}}
\bibcite{Alwall:2011uj}{{2}{2011}{{Alwall et~al.}}{{Alwall, Herquet, Maltoni, Mattelaer, and Stelzer}}}
\bibcite{2017arXiv170104862A}{{3}{2017}{{{Arjovsky} and {Bottou}}}{{}}}
\bibcite{2017arXiv170107875A}{{4}{2017}{{{Arjovsky} et~al.}}{{{Arjovsky}, {Chintala}, and {Bottou}}}}
\bibcite{beaumont2002approximate}{{5}{2002}{{Beaumont et~al.}}{{Beaumont, Zhang, and Balding}}}
\bibcite{bernton2017inference}{{6}{2017}{{Bernton et~al.}}{{Bernton, Jacob, Gerber, and Robert}}}
\bibcite{buesingstochastic}{{7}{2016}{{Buesing et~al.}}{{Buesing, Weber, and Mohamed}}}
\bibcite{cranmer2016experiments}{{8}{2016}{{Cranmer et~al.}}{{Cranmer, Pavez, Louppe, and Brooks}}}
\bibcite{cranmer2015approximating}{{9}{2015}{{Cranmer et~al.}}{{Cranmer, Pavez, and Louppe}}}
\bibcite{cuturi2013sinkhorn}{{10}{2013}{{Cuturi}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Summary}{12}{section.0.7}}
\bibcite{donahue2016adversarial}{{11}{2016}{{Donahue et~al.}}{{Donahue, Kr{\"a}henb{\"u}hl, and Darrell}}}
\bibcite{dumoulin2016adversarially}{{12}{2016}{{Dumoulin et~al.}}{{Dumoulin, Belghazi, Poole, Lamb, Arjovsky, Mastropietro, and Courville}}}
\bibcite{2016arXiv161110242D}{{13}{2016}{{{Dutta} et~al.}}{{{Dutta}, {Corander}, {Kaski}, and {Gutmann}}}}
\bibcite{genevay2016stochastic}{{14}{2016}{{Genevay et~al.}}{{Genevay, Cuturi, Peyr{\'e}, and Bach}}}
\bibcite{goodfellow2014generative}{{15}{2014}{{Goodfellow et~al.}}{{Goodfellow, Pouget-Abadie, Mirza, Xu, Warde-Farley, Ozair, Courville, and Bengio}}}
\bibcite{2017arXiv171100123G}{{16}{2017}{{{Grathwohl} et~al.}}{{{Grathwohl}, {Choi}, {Wu}, {Roeder}, and {Duvenaud}}}}
\bibcite{2017arXiv170400028G}{{17}{2017}{{{Gulrajani} et~al.}}{{{Gulrajani}, {Ahmed}, {Arjovsky}, {Dumoulin}, and {Courville}}}}
\bibcite{gutmann2012noise}{{18}{2012}{{Gutmann and Hyv{\"a}rinen}}{{}}}
\bibcite{gutmann2017likelihood}{{19}{2017}{{Gutmann et~al.}}{{Gutmann, Dutta, Kaski, and Corander}}}
\bibcite{2017arXiv170208235H}{{20}{2017}{{Husz{\'a}r}}{{}}}
\bibcite{2014arXiv1412.6980K}{{21}{2014}{{{Kingma} and {Ba}}}{{}}}
\bibcite{DBLP:journals/corr/KingmaW13}{{22}{2013}{{Kingma and Welling}}{{}}}
\bibcite{marin2012approximate}{{23}{2012}{{Marin et~al.}}{{Marin, Pudlo, Robert, and Ryder}}}
\bibcite{marjoram2003markov}{{24}{2003}{{Marjoram et~al.}}{{Marjoram, Molitor, Plagnol, and Tavar{\'e}}}}
\bibcite{2017arXiv171203353M}{{25}{2017}{{{McCarthy} et~al.}}{{{McCarthy}, {Rodriguez}, and {Minchole}}}}
\bibcite{meeds2015hamiltonian}{{26}{2015}{{Meeds et~al.}}{{Meeds, Leenders, and Welling}}}
\bibcite{DBLP:journals/corr/MeschederNG17}{{27}{2017}{{Mescheder et~al.}}{{Mescheder, Nowozin, and Geiger}}}
\bibcite{2016arXiv161003483M}{{28}{2016}{{{Mohamed} and {Lakshminarayanan}}}{{}}}
\bibcite{montavon2016wasserstein}{{29}{2016}{{Montavon et~al.}}{{Montavon, M{\"u}ller, and Cuturi}}}
\bibcite{2016arXiv161009033R}{{30}{2016}{{{Ranganath} et~al.}}{{{Ranganath}, {Altosaar}, {Tran}, and {Blei}}}}
\bibcite{rosca2017variational}{{31}{2017}{{Rosca et~al.}}{{Rosca, Lakshminarayanan, Warde-Farley, and Mohamed}}}
\bibcite{rubin1984}{{32}{1984}{{Rubin}}{{}}}
\bibcite{sisson2011likelihood}{{33}{2011}{{Sisson and Fan}}{{}}}
\bibcite{sisson2007sequential}{{34}{2007}{{Sisson et~al.}}{{Sisson, Fan, and Tanaka}}}
\bibcite{2012arXiv1212.4507S}{{35}{2012}{{{Staines} and {Barber}}}{{}}}
\bibcite{staines2013optimization}{{36}{2013}{{Staines and Barber}}{{}}}
\bibcite{2017arXiv170208896T}{{37}{2017}{{{Tran} et~al.}}{{{Tran}, {Ranganath}, and {Blei}}}}
\bibcite{tran2017variational}{{38}{2017}{{Tran et~al.}}{{Tran, Nott, and Kohn}}}
\bibcite{2011arXiv1106.4487W}{{39}{2011}{{{Wierstra} et~al.}}{{{Wierstra}, {Schaul}, {Glasmachers}, {Sun}, and {Schmidhuber}}}}
\newlabel{jmlrend}{{7}{14}{end of Adversarial Variational Optimization}{section*.2}{}}
